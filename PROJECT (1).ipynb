{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGlMkYw0UO2x",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets openai accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets fsspec huggingface_hub\n",
        "\n",
        "# STEP 2: Load Dataset â€“ GSM8K\n",
        "from datasets import load_dataset\n",
        "\n",
        "gsm8k = load_dataset(\"gsm8k\", \"main\", download_mode=\"force_redownload\")\n",
        "sample_problems = gsm8k['train'].select(range(10))\n",
        "\n",
        "print(sample_problems)\n",
        "\n",
        "# STEP 3: Load Models\n",
        "from transformers import pipeline\n",
        "\n",
        "# Use device_map=\"auto\" for automatic GPU/CPU allocation\n",
        "qwen = pipeline(\"text-generation\", model=\"Qwen/Qwen1.5-7B-Chat\", device_map=\"auto\")\n",
        "deepseek = pipeline(\"text-generation\", model=\"deepseek-ai/deepseek-math-7b-base\", device_map=\"auto\")\n",
        "\n",
        "# STEP 4: Prompting Functions\n",
        "\n",
        "def direct_prompt(question):\n",
        "    return f\"Q: {question}\\nA:\"\n",
        "\n",
        "def cot_prompt(question):\n",
        "    return f\"Q: {question}\\nLet's think step by step.\\nA:\"\n",
        "\n",
        "# STEP 5: Run Experiments\n",
        "\n",
        "def run_agent(model_pipeline, question, use_cot=True):\n",
        "    prompt = cot_prompt(question) if use_cot else direct_prompt(question)\n",
        "    result = model_pipeline(prompt, max_new_tokens=256)[0]['generated_text']\n",
        "    return result\n",
        "\n",
        "# STEP 6: Log Results for Evaluation\n",
        "\n",
        "results = []\n",
        "\n",
        "for item in sample_problems:\n",
        "    q = item['question']\n",
        "    a = item['answer']\n",
        "    for model_name, model_pipe in [(\"Qwen\", qwen), (\"DeepSeek\", deepseek)]:\n",
        "        for mode in [\"direct\", \"cot\"]:\n",
        "            out = run_agent(model_pipe, q, use_cot=(mode == \"cot\"))\n",
        "            results.append({\n",
        "                \"model\": model_name,\n",
        "                \"mode\": mode,\n",
        "                \"question\": q,\n",
        "                \"expected\": a,\n",
        "                \"response\": out\n",
        "            })\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vBmeCI5Ujt_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}